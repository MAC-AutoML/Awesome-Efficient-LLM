# Awesome-Efficient-LLM

# Toxonomy and Papers
- [Sparsity and Pruning](#Sparsity-and-Pruning)
  - [Unstructured Pruning](#Unstructured-Pruning)
  - [Semi-structured Pruning](#Semi-structured-Pruning)
  - [Structured Pruning](#Structured-Pruning)
  - [Activation Sparsity](#Activation-Sparsity)
  - [Joint Sparsification and Quantization](#Joint-Sparsification-and-Quantization)
- [Quantization](#Quantization)
  - [LLM Quantization](#LLM-Quantization)
  - [VLM Quantization](#VLM-Quantization)
- [Knowledge Distillation](#Knowledge-Distillation)
- [Low-Rank Decomposition](#Low-Rank-Decomposition)
- [KV Cache Compression](#KV-Cache-Compression)
- [Speculative Decoding](#Speculative-Decoding)
- [Diffusion Models](#Diffusion-Models)

---
# Sparsity and Pruning
## Unstructured Pruning   
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                     |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2023 | SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot | ICML 2023| [Link](https://arxiv.org/pdf/2301.00774) |         [Link](https://github.com/IST-DASLab/sparsegpt) ![](https://img.shields.io/github/stars/IST-DASLab/sparsegpt.svg?style=social) |
| 2023 | A Simple and Effective Pruning Approach for Large Language Models | ICLR 2024| [Link](https://arxiv.org/abs/2306.11695) |         [Link](https://github.com/locuslab/wanda) ![](https://img.shields.io/github/stars/locuslab/wanda.svg?style=social) |
| 2023 | Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs | ICLR 2024| [Link](https://arxiv.org/abs/2310.08915) |         [Link](https://github.com/zyxxmu/DSnoT) ![](https://img.shields.io/github/stars/zyxxmu/DSnoT.svg?style=social) |
| 2023 | Compressing LLMs: The Truth is Rarely Pure and Never Simple| ICLR 2024| [Link](https://arxiv.org/abs/2310.01382) |         [Link](https://github.com/VITA-Group/llm-kick) ![](https://img.shields.io/github/stars/VITA-Group/llm-kick.svg?style=social) |
| 2023 | Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly andMonotonically Impairs “Difficult" Downstream Tasks in LLMs| ICML 2024| [Link](https://arxiv.org/abs/2310.02277) |         [Link](https://github.com/VITA-Group/Junk_DNA_Hypothesis) ![](https://img.shields.io/github/stars/VITA-Group/Junk_DNA_Hypothesis.svg?style=social) |
| 2023 | Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity| ICML 2024| [Link](https://arxiv.org/abs/2310.05175) |         [Link](https://github.com/luuyin/OWL) ![](https://img.shields.io/github/stars/luuyin/OWL.svg?style=social) |
| 2023 |The LLM Surgeon| ICLR 2024| [Link](https://arxiv.org/pdf/2312.17244) |         [Link](https://github.com/Qualcomm-AI-research/llm-surgeon) ![](https://img.shields.io/github/stars/Qualcomm-AI-research/llm-surgeon.svg?style=social) |
| 2024 | BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation| ICLR 2024| [Link](https://arxiv.org/abs/2402.16880) |         [Link](https://github.com/OpenGVLab/LLMPrune-BESA) ![](https://img.shields.io/github/stars/OpenGVLab/LLMPrune-BESA.svg?style=social) |
| 2024 |Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models| ICLR 2024| [Link](https://openreview.net/forum?id=Tr0lPx9woF) |         [Link](https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning) ![](https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning.svg?style=social) |
| 2024 |Fast and Optimal Weight Update for Pruned Large Language Models| TMLR 2024| [Link](https://arxiv.org/abs/2401.02938) |[Link](https://github.com/fmfi-compbio/admm-pruning) ![](https://img.shields.io/github/stars/fmfi-compbio/admm-pruning.svg?style=social) |
| 2024 |SparseLLM: Towards Global Pruning for Pre-trained Language Models| NeurIPS 2024| [Link](https://arxiv.org/abs/2402.17946) |[Link](https://github.com/BaiTheBest/SparseLLM) ![](https://img.shields.io/github/stars/BaiTheBest/SparseLLM.svg?style=social) |
| 2024 |Shears: Unstructured Sparsity with Neural Low-rank Adapter Search| NAACL 2024| [Link](https://arxiv.org/abs/2404.10934) |[Link](https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning) ![](https://img.shields.io/github/stars/IntelLabs/Hardware-Aware-Automated-Machine-Learning.svg?style=social) |
| 2024 |COPAL: Continual Pruning in Large Language Generative Models| ICML 2024| [Link](https://arxiv.org/abs/2405.02347) |N/A|
| 2024 |Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models| ICML 2024| [Link](https://arxiv.org/abs/2406.02924) |[Link](https://github.com/pprp/Pruner-Zero) ![](https://img.shields.io/github/stars/pprp/Pruner-Zero.svg?style=social) |
| 2024 |ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2406.07831) |[Link](https://github.com/mazumder-lab/ALPS) ![](https://img.shields.io/github/stars/mazumder-lab/ALPS.svg?style=social) |
| 2024 |AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2410.10912) |[Link](https://github.com/haiquanlu/AlphaPruning) ![](https://img.shields.io/github/stars/haiquanlu/AlphaPruning.svg?style=social) |
| 2024 |Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization|EMNLP 2024| [Link](https://arxiv.org/abs/2406.15524) |[Link](https://github.com/LOG-postech/rethinking-LLM-pruning) ![](https://img.shields.io/github/stars/LOG-postech/rethinking-LLM-pruning.svg?style=social) |
| 2024 |ALS: Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment|NeurIPS 2024| [Link](https://openreview.net/forum?id=Jup0qZxH7U) |[Link](https://github.com/lliai/ALS) ![](https://img.shields.io/github/stars/lliai/ALS.svg?style=social) |
| 2024 |Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models|NeurIPS 2024| [Link](https://openreview.net/forum?id=rgtrYVC9n4) |[Link](https://github.com/lliai/DSA) ![](https://img.shields.io/github/stars/lliai/DSA.svg?style=social) |
| 2024 |EvoPress: Accurate Dynamic Model Compression via Evolutionary Search|ICML 2025| [Link](https://arxiv.org/abs/2410.14649) |[Link](https://github.com/IST-DASLab/EvoPress) ![](https://img.shields.io/github/stars/IST-DASLab/EvoPress.svg?style=social) |
| 2024 |Pruning Foundation Models for High Accuracy without Retraining|EMNLP 2024 findings| [Link](https://arxiv.org/abs/2410.15567) |[Link](https://github.com/piuzha/APT) ![](https://img.shields.io/github/stars/piuzha/APT.svg?style=social) |
| 2024 |Beware of Calibration Data for Pruning Large Language Models|ICLR 2025| [Link](https://arxiv.org/abs/2410.17711) |[Link](https://github.com/Dereck0602/calibration_data) ![](https://img.shields.io/github/stars/Dereck0602/calibration_data.svg?style=social) |
| 2024 |Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix|ICLR 2025| [Link](https://arxiv.org/abs/2410.11261) |N/A |
| 2024 |OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition|ICLR 2025| [Link](https://arxiv.org/abs/2409.13652) |[Link](https://github.com/stephenqz/OATS) ![](https://img.shields.io/github/stars/stephenqz/OATS.svg?style=social) |
| 2024 |Wasserstein Distances, Neuronal Entanglement, and Sparsity|ICLR 2025| [Link](https://arxiv.org/abs/2405.15756) |[Link](https://github.com/Shavit-Lab/Sparse-Expansion) ![](https://img.shields.io/github/stars/Shavit-Lab/Sparse-Expansion.svg?style=social) |
| 2024 |Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization|ICLR 2025| [Link](https://arxiv.org/abs/2409.18850) |[Link](https://github.com/usamec/double_sparse) ![](https://img.shields.io/github/stars/usamec/double_sparse.svg?style=social) |
| 2025 |Adaptive Pruning of Pretrained Transformer via Differential Inclusions|ICLR 2025| [Link](https://arxiv.org/abs/2501.03289) |[Link](https://github.com/yizhuoDi/Solution-Path-Pruning) ![](https://img.shields.io/github/stars/yizhuoDi/Solution-Path-Pruning.svg?style=social) |
| 2025 |Dynamic Low-Rank Sparse Adaptation for Large Language Models|ICLR 2025| [Link](https://arxiv.org/abs/2502.14816) |[Link](https://github.com/wzhuang-xmu/LoSA) ![](https://img.shields.io/github/stars/wzhuang-xmu/LoSA.svg?style=social) |
| 2025 |Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models|ICML 2025| [Link](https://arxiv.org/abs/2501.19090) |[Link](https://github.com/biomedical-cybernetics/pivoting-factorization) ![](https://img.shields.io/github/stars/biomedical-cybernetics/pivoting-factorization.svg?style=social) |
| 2025 |BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation|ICML 2025| [Link](https://openreview.net/forum?id=YrCvW1Hx7g) |N/A |
| 2025 |SAFE: Finding Sparse and Flat Minima to Improve Pruning|ICML 2025| [Link](https://arxiv.org/abs/2506.06866) |[Link](https://github.com/LOG-postech/safe-torch) ![](https://img.shields.io/github/stars/LOG-postech/safe-torch.svg?style=social) |
| 2025 |An Efficient Pruner for Large Language Model with Theoretical Guarantee|ICML 2025| [Link](https://openreview.net/forum?id=nh9mBCYeF7) |N/A |
| 2025 |Targeted Low-rank Refinement: Enhancing Sparse Language Models with Precision|ICML 2025| [Link](https://openreview.net/forum?id=S0ncZdwcLt) |N/A |
| 2025 |Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective|ICML 2025| [Link](https://openreview.net/forum?id=otNB7BzsiR) |[Link](https://github.com/wzhuang-xmu/ATP) ![](https://img.shields.io/github/stars/wzhuang-xmu/ATP.svg?style=social) |
| 2025 |DLP: Dynamic Layerwise Pruning in Large Language Models|ICML 2025| [Link](https://arxiv.org/abs/2505.23807) | [Link](https://github.com/ironartisan/DLP) ![](https://img.shields.io/github/stars/ironartisan/DLP.svg?style=social)|
| 2025 |3BASiL: An Algorithmic Framework for Sparse plus Low-Rank Compression of LLMs|NeurIPS 2025| [Link](https://openreview.net/forum?id=byNNv5Et10) |[Link](https://github.com/mazumder-lab/3BASiL) ![](https://img.shields.io/github/stars/mazumder-lab/3BASiL.svg?style=social) |
| 2025 |DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs|NeurIPS 2025| [Link](https://arxiv.org/abs/2506.20194) |[Link](https://github.com/RuokaiYin/DuoGPT) ![](https://img.shields.io/github/stars/RuokaiYin/DuoGPT.svg?style=social) |
| 2025 |Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models|NeurIPS 2025| [Link](https://openreview.net/forum?id=CA1xVSvn72) |N/A |
| 2025 |DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration|NeurIPS 2025| [Link](https://arxiv.org/abs/2505.23049) |[Link](https://github.com/Axel-gu/DenoiseRotator) ![](https://img.shields.io/github/stars/Axel-gu/DenoiseRotator.svg?style=social) |
| 2025 |Multi-Objective One-Shot Pruning for Large Language Models|NeurIPS 2025| [Link](https://openreview.net/forum?id=aNpj43Uh35) |N/A |

## Semi-structured Pruning 
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                     |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2024 |Pruning Large Language Models with Semi-Structural Adaptive Sparse Training|AAAI 2025| [Link](https://arxiv.org/abs/2407.20584) |[Link](https://github.com/thu-ml/Adaptive-Sparse-Trainer) ![](https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer.svg?style=social) |
| 2024 |MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2409.17481) |[Link](https://github.com/NVlabs/MaskLLM) ![](https://img.shields.io/github/stars/NVlabs/MaskLLM.svg?style=social) |
| 2025 | ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for Pretrained LLMs|ICML 2025| [Link](https://arxiv.org/abs/2502.00258) |[Link](https://github.com/amazon-science/ProxSparse) ![](https://img.shields.io/github/stars/amazon-science/ProxSparse.svg?style=social) |
| 2025 |PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models|NeurIPS 2025| [Link](https://arxiv.org/abs/2510.10136) |[Link](https://github.com/lanchengzou/PermLLM) ![](https://img.shields.io/github/stars/lanchengzou/PermLLM.svg?style=social) |
| 2025 |TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks|NeurIPS 2025| [Link](https://arxiv.org/abs/2505.23949) |[Link](https://github.com/mazumder-lab/TSENOR) ![](https://img.shields.io/github/stars/mazumder-lab/TSENOR.svg?style=social) |

## Structured Pruning 
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                     |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2023 |LLM-Pruner: On the Structural Pruning of Large Language Models| NeurIPS 2023| [Link](https://arxiv.org/abs/2305.11627) |  [Link](https://github.com/horseee/LLM-Pruner) ![](https://img.shields.io/github/stars/horseee/LLM-Pruner.svg?style=social) |
| 2023 |Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning| ACL 2024 Findings| [Link](https://arxiv.org/abs/2305.18403) |  [Link](https://github.com/aim-uofa/LoRAPrune) ![](https://img.shields.io/github/stars/aim-uofa/LoRAPrune.svg?style=social) |
| 2023 |Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning| ICLR 2024| [Link](https://arxiv.org/abs/2310.06694) |         [Link](https://github.com/princeton-nlp/LLM-Shearing) ![](https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing.svg?style=social) |
| 2023 |Fluctuation-based Adaptive Structured Pruning for Large Language Models| AAAI 2024| [Link](https://arxiv.org/abs/2312.11983) |         [Link](https://github.com/CASIA-IVA-Lab/FLAP) ![](https://img.shields.io/github/stars/CASIA-IVA-Lab/FLAP.svg?style=social) |
| 2024 |SliceGPT: Compress Large Language Models by Deleting Rows and Columns| ICLR 2024| [Link](https://arxiv.org/abs/2401.15024) |         [Link](https://github.com/microsoft/TransformerCompression) ![](https://img.shields.io/github/stars/microsoft/TransformerCompression.svg?style=social) |
| 2024 |APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference| ICML 2024| [Link](https://arxiv.org/abs/2401.12200) |         [Link](https://github.com/ROIM1998/APT) ![](https://img.shields.io/github/stars/ROIM1998/APT.svg?style=social) |
| 2024 |Shortened LLaMA: A Simple Depth Pruning for Large Language Models| ICLR 2024 workshop| [Link](https://arxiv.org/abs/2402.02834) |[Link](https://github.com/Nota-NetsPresso/shortened-llm) ![](https://img.shields.io/github/stars/Nota-NetsPresso/shortened-llm.svg?style=social) |
| 2024 |LaCo: Large Language Model Pruning via Layer Collapse| EMNLP 2024 Findings| [Link](https://arxiv.org/abs/2402.02834) |[Link](https://github.com/yangyifei729/LaCo) ![](https://img.shields.io/github/stars/yangyifei729/LaCo.svg?style=social) |
| 2024 |Shortgpt: Layers in large language models are more redundant than you expect|ACL 2025 Findings| [Link](https://arxiv.org/abs/2403.03853) |[Link](https://github.com/sramshetty/ShortGPT) ![](https://img.shields.io/github/stars/sramshetty/ShortGPT.svg?style=social) |
| 2024 |Streamlining Redundant Layers to Compress Large Language Models|ICLR 2025| [Link](https://arxiv.org/abs/2403.19135) |[Link](https://github.com/RUCKBReasoning/LLM-Streamline) ![](https://img.shields.io/github/stars/RUCKBReasoning/LLM-Streamline.svg?style=social) |
| 2024 |LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models|ICML 2024| [Link](https://arxiv.org/abs/2404.09695) |[Link](https://github.com/lihuang258/LoRAP) ![](https://img.shields.io/github/stars/lihuang258/LoRAP.svg?style=social) |
| 2024 |SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks|ICML 2024| [Link](https://arxiv.org/abs/2402.09025) |[Link](https://github.com/jiwonsong-dev/SLEB) ![](https://img.shields.io/github/stars/jiwonsong-dev/SLEB.svg?style=social) |
| 2024 |Pruning as a Domain-specific LLM Extractor| NAACL 2024 Findings| [Link](https://arxiv.org/abs/2405.06275) |[Link](https://github.com/psunlpgroup/D-Pruner) ![](https://img.shields.io/github/stars/psunlpgroup/D-Pruner.svg?style=social) |
| 2024 |Compact Language Models via Pruning and Knowledge Distillation| NeurIPS 2024|[Link](https://arxiv.org/abs/2407.14679) |[Link](https://github.com/NVlabs/Minitron) ![](https://img.shields.io/github/stars/NVlabs/Minitron.svg?style=social) |
| 2024 |Bypass Back-propagation: Optimization-based Structural Pruning for Large Language Models via Policy Gradient| ACL 2025| [Link](https://arxiv.org/abs/2406.10576) |[Link](https://github.com/ethanygao/backprop-free_LLM_pruning) ![](https://img.shields.io/github/stars/ethanygao/backprop-free_LLM_pruning.svg?style=social) |
| 2024 |BlockPruner: Fine-grained Pruning for Large Language Models| ACL 2025 Findings| [Link](https://arxiv.org/abs/2406.10594) |[Link](https://github.com/MrGGLS/BlockPruner) ![](https://img.shields.io/github/stars/MrGGLS/BlockPruner.svg?style=social) |
| 2024 |RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned LLMs via Performance Model| NAACL 2024 Findings| [Link](https://aclanthology.org/2025.findings-naacl.321) |N/A |
| 2024 |Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging| EMNLP 2024| [Link](https://arxiv.org/abs/2406.16330) |N/A |
| 2024 |Finding Transformer Circuits with Edge Pruning|NeurIPS 2024| [Link](https://arxiv.org/abs/2406.16778) |[Link](https://github.com/princeton-nlp/Edge-Pruning) ![](https://img.shields.io/github/stars/princeton-nlp/Edge-Pruning.svg?style=social) |
| 2024 |MoDeGPT: Modular Decomposition for Large Language Model Compression|ICLR 2025| [Link](https://arxiv.org/abs/2408.09632) |[Link](https://github.com/cbacary/MoDeGPT) ![](https://img.shields.io/github/stars/cbacary/MoDeGPT.svg?style=social) |
| 2024 |The Unreasonable Ineffectiveness of the Deeper Layers|ICLR 2025| [Link](https://arxiv.org/abs/2403.17887) | N/A|
| 2024 |PAT: Pruning-Aware Tuning for Large Language Models|AAAI 2025| [Link](https://arxiv.org/abs/2408.14721) |[Link](https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning) ![](https://img.shields.io/github/stars/kriskrisliu/PAT_Pruning-Aware-Tuning.svg?style=social) |
| 2024 |DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2410.11988) |[Link](https://github.com/ZhengaoLi/DISP-LLM-Dimension-Independent-Structural-Pruning) ![](https://img.shields.io/github/stars/ZhengaoLi/DISP-LLM-Dimension-Independent-Structural-Pruning.svg?style=social) |
| 2024 |Search for Efficient Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2409.17372) |[Link](https://github.com/shawnricecake/search-llm) ![](https://img.shields.io/github/stars/shawnricecake/search-llm.svg?style=social) |
| 2024 |SlimGPT: Layer-wise Structured Pruning for Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2412.18110) |N/A |
| 2024 |Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy |EMNLP 2024 Findings| [Link](https://arxiv.org/abs/2411.03513) |[Link](https://github.com/RazvanDu/DynamicSlicing) ![](https://img.shields.io/github/stars/RazvanDu/DynamicSlicing.svg?style=social) |
| 2024 |LEMON: Reviving Stronger and Smaller LMs from Larger LMs with Linear Parameter Fusion|ACL 2024| [Link](https://aclanthology.org/2024.acl-long.434) | N/A|
| 2024 |TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs|ACL 2025| [Link](https://arxiv.org/abs/2412.11242) | [Link](https://github.com/snyhlxde1/TrimLLM) ![](https://img.shields.io/github/stars/snyhlxde1/TrimLLM.svg?style=social)|
| 2024 |DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization|ACL 2025| [Link](https://arxiv.org/abs/2411.14055) | [Link](https://github.com/hexuandeng/DRPruning) ![](https://img.shields.io/github/stars/hexuandeng/DRPruning.svg?style=social)|
| 2025 |You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning|ICLR 2025| [Link](https://arxiv.org/abs/2501.15296) | [Link](https://github.com/LCS2-IIITD/PruneNet) ![](https://img.shields.io/github/stars/LCS2-IIITD/PruneNet.svg?style=social)|
| 2025 |LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing|ICLR 2025| [Link](https://openreview.net/forum?id=AyC4uxx2HW) |N/A|
| 2025 |Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing|ICLR 2025| [Link](https://arxiv.org/abs/2502.15618) | [Link](https://github.com/Qi-Le1/Probe_Pruning) ![](https://img.shields.io/github/stars/Qi-Le1/Probe_Pruning.svg?style=social)|
| 2025 |You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning|ICLR 2025| [Link](https://arxiv.org/abs/2501.15296) | [Link](https://github.com/LCS2-IIITD/PruneNet) ![](https://img.shields.io/github/stars/LCS2-IIITD/PruneNet.svg?style=social)|
| 2025 |Instruction-Following Pruning for Large Language Models|ICML 2025| [Link](https://arxiv.org/abs/2501.02086) | N/A |
| 2025 |Let LLM Tell What to Prune and How Much to Prune|ICML 2025| [Link](https://openreview.net/forum?id=zFR5aWGaUs) | [Link](https://github.com/yangmzevery/PruneLLM) ![](https://img.shields.io/github/stars/yangmzevery/PruneLLM.svg?style=social)|
| 2025 |Olica: Efficient Structured Pruning of Large Language Models without Retraining|ICML 2025| [Link](https://arxiv.org/abs/2506.08436) | [Link](https://github.com/BetterTMrR/LLM-Olica) ![](https://img.shields.io/github/stars/BetterTMrR/LLM-Olica.svg?style=social)|
| 2025 |Prompt-based Depth Pruning of Large Language Models|ICML 2025| [Link](https://arxiv.org/abs/2502.04348) | [Link](https://github.com/tada0347/PuDDing) ![](https://img.shields.io/github/stars/tada0347/PuDDing.svg?style=social)|
| 2025 |Týr-the-Pruner: Structural Pruning LLMs via Global Sparsity Distribution Optimization|NeurIPS 2025| [Link](https://arxiv.org/abs/2503.09657) |[Link](https://github.com/AMD-AGI/Tyr-the-Pruner) ![](https://img.shields.io/github/stars/AMD-AGI/Tyr-the-Pruner.svg?style=social) |
| 2025 |A Simple Linear Patch Revives Layer-Pruned Large Language Models|NeurIPS 2025| [Link](https://arxiv.org/abs/2505.24680) |[Link](https://github.com/chenxinrui-tsinghua/LinearPatch) ![](https://img.shields.io/github/stars/chenxinrui-tsinghua/LinearPatch.svg?style=social) |
| 2025 |ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization|NeurIPS 2025| [Link](https://arxiv.org/abs/2505.02819) |[Link](https://github.com/mts-ai/ReplaceMe) ![](https://img.shields.io/github/stars/mts-ai/ReplaceMe.svg?style=social) |
| 2025 |Restoring Pruned Large Language Models via Lost Component Compensation|NeurIPS 2025| [Link](https://arxiv.org/abs/2510.21834) |[Link](https://github.com/zijian678/restorelcc) ![](https://img.shields.io/github/stars/zijian678/restorelcc.svg?style=social) |

## Activation Sparsity
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                        |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2023 |Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time| ICML 2024| [Link](https://arxiv.org/abs/2310.17157) |  [Link](https://github.com/FMInference/DejaVu) ![](https://img.shields.io/github/stars/FMInference/DejaVu.svg?style=social) |
| 2023 |ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models| ICLR 2024| [Link](https://arxiv.org/abs/2310.04564) | 	N/A |
| 2024 |CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models|COLM 2024|[Link](https://arxiv.org/abs/2404.08763) |[Link](https://github.com/ScalingIntelligence/CATS) ![](https://img.shields.io/github/stars/ScalingIntelligence/CATS.svg?style=social) |
| 2024 |ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models|EMNLP 2024|[Link](https://arxiv.org/abs/2406.16635) |[Link](https://github.com/abdelfattah-lab/shadow_llm) ![](https://img.shields.io/github/stars/abdelfattah-lab/shadow_llm.svg?style=social) |
| 2024 |Training-Free Activation Sparsity in Large Language Models|ICLR 2025|[Link](https://arxiv.org/abs/2408.14690) | [Link](https://github.com/FasterDecoding/TEAL) ![](https://img.shields.io/github/stars/FasterDecoding/TEAL.svg?style=social) |
| 2024 |Sparsing Law: Towards Large Language Models with Greater Activation Sparsity|ICML 2025|[Link](https://arxiv.org/abs/2411.02335) |[Link](https://github.com/thunlp/SparsingLaw) ![](https://img.shields.io/github/stars/thunlp/SparsingLaw.svg?style=social) |
| 2025 |La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation|ICML 2025|[Link](https://openreview.net/forum?id=1b6NNpFYI4) |N/A |
| 2025 |R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference|ICLR 2025|[Link](https://arxiv.org/abs/2504.19449) | [Link](https://github.com/VITA-Group/R-Sparse) ![](https://img.shields.io/github/stars/VITA-Group/R-Sparse.svg?style=social) |
| 2024 |Sirius: Contextual Sparsity with Correction for Efficient LLMs|NeurIPS 2024| [Link](https://arxiv.org/abs/2409.03856) |[Link](https://github.com/Infini-AI-Lab/Sirius) ![](https://img.shields.io/github/stars/Infini-AI-Lab/Sirius.svg?style=social) |
| 2024 |Learn To be Efficient: Build Structured Sparsity in Large Language Models|NeurIPS 2024| [Link](https://arxiv.org/abs/2402.06126) |[Link](https://github.com/haizhongzheng/LTE) ![](https://img.shields.io/github/stars/haizhongzheng/LTE.svg?style=social) |
| 2025 |Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity|NeurIPS 2025| [Link](https://arxiv.org/abs/2505.14884) |[Link](https://github.com/susavlsh10/Polar-Sparsity) ![](https://img.shields.io/github/stars/susavlsh10/Polar-Sparsity.svg?style=social) |

## Joint Sparsification and Quantization
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                     |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2024 |Effective Interplay between Sparsity and Quantization: From Theory to Practice|ICLR 2025| [Link](https://arxiv.org/abs/2405.20935) |[Link](https://github.com/parsa-epfl/quantization-sparsity-interplay) ![](https://img.shields.io/github/stars/parsa-epfl/quantization-sparsity-interplay.svg?style=social) |
| 2024 | Compressing large language models by joint sparsification and quantization|ICML 2024| [Link](https://openreview.net/forum?id=sCGRhnuMUJ) |[Link](https://github.com/uanu2002/JSQ) ![](https://img.shields.io/github/stars/uanu2002/JSQ.svg?style=social) |
| 2024 |SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression|ICML 2025| [Link](https://arxiv.org/abs/2410.09615) |[Link](https://github.com/Paramathic/slim) ![](https://img.shields.io/github/stars/Paramathic/slim.svg?style=social) |
| 2025 |Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs |arxiv 2025| [Link](https://arxiv.org/abs/2410.09615) |[Link](https://github.com/csguoh/OBR) ![](https://img.shields.io/github/stars/csguoh/OBR.svg?style=social) |

---
# Quantization
## LLM Quantization
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                        |
| ---- | ----------------------------------------------------------------------- | :------ | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2023 | GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers | ICLR   2023 | [Link](https://arxiv.org/abs/2210.17323) |         [Link](https://github.com/IST-DASLab/gptq) ![](https://img.shields.io/github/stars/IST-DASLab/gptq.svg?style=social) |
| 2025 | OSTQuant: Refining Large Language Model Quantization with <br/>Orthogonal and Scaling Transformations for Better Distribution Fitting | ICLR   2025 | [Link](https://arxiv.org/abs/2405.16406) | [Link](https://github.com/BrotherHappy/OSTQuant) ![](https://img.shields.io/github/stars/BrotherHappy/OSTQuant.svg?style=social) |
| 2025 | SpinQuant: LLM quantization with learned rotations | ICLR   2025 | [Link](https://arxiv.org/abs/2501.13987) | [Link](https://github.com/facebookresearch/SpinQuant) ![](https://img.shields.io/github/stars/facebookresearch/SpinQuant.svg?style=social) |
| 2022 | SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | ICML  2023 | [Link](https://arxiv.org/abs/2211.10438) | [Link](https://github.com/mit-han-lab/smoothquant) ![](https://img.shields.io/github/stars/mit-han-lab/smoothquant.svg?style=social) |
| 2023 | AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration | MLSys 2024 | [Link](https://arxiv.org/abs/2306.00978) | [Link](https://github.com/mit-han-lab/llm-awq) ![](https://img.shields.io/github/stars/mit-han-lab/llm-awq.svg?style=social) |
| 2024 | QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks | ICML  2024 | [Link](https://arxiv.org/abs/2402.04396) | [Link](https://github.com/Cornell-RelaxML/quip-sharp) ![](https://img.shields.io/github/stars/Cornell-RelaxML/quip-sharp.svg?style=social) |
| 2025 | QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving | MLSys 2025 | [Link](https://arxiv.org/abs/2405.04532) |  [Link](https://github.com/mit-han-lab/omniserve) ![](https://img.shields.io/github/stars/mit-han-lab/omniserve.svg?style=social) |
| 2024 | QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs         | NeurIPS 2024 | [Link](https://arxiv.org/abs/2404.00456) |[Link](https://github.com/spcl/QuaRot)![](https://img.shields.io/github/stars/spcl/QuaRot.svg?style=social)|
| 2024 | Atom: Low-bit Quantization for Efficient and Accurate LLM Serving | MLSys 2024 | [Link](https://arxiv.org/abs/2404.00456) |[Link](https://github.com/efeslab/Atom)![](https://img.shields.io/github/stars/efeslab/Atom.svg?style=social) |
| 2024 | OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models | ICLR 2024 | [Link](https://arxiv.org/abs/2308.13137) |[Link](https://github.com/OpenGVLab/OmniQuant)![](https://img.shields.io/github/stars/OpenGVLab/OmniQuant.svg?style=social)|
| 2023 | QuIP: 2-Bit Quantization of Large Language Models With Guarantees | NeurIPS 2023 | [Link](https://arxiv.org/abs/2307.13304) | [Link](https://github.com/AlpinDale/QuIP-for-Llama?tab=readme-ov-file)![](https://img.shields.io/github/stars/AlpinDale/QuIP-for-Llama.svg?style=social) |
| 2022 | LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale | NeurIPS 2022 | [Link](https://arxiv.org/abs/2208.07339) | [Link](https://github.com/tloen/llama-int8)![](https://img.shields.io/github/stars/tloen/llama-int8.svg?style=social) |
| 2023 | Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling | EMNLP 2023 | [Link](https://arxiv.org/abs/2304.09145) | [Link](https://github.com/ModelTC/Outlier_Suppression_Plus)![](https://img.shields.io/github/stars/ModelTC/Outlier_Suppression_Plus.svg?style=social) |
| 2025 | GPTAQ: Efficient Finetuning-Free Quantization for Asyetric Calibration | ICML 2025 | [Link](https://arxiv.org/pdf/2504.02692) | [Link](https://github.com/Intelligent-Computing-Lab-Panda/GPTAQ)![](https://img.shields.io/github/stars/Intelligent-Computing-Lab-Panda/GPTAQ.svg?style=social) |
| 2024 | MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization | NeurIPS 2024 | [Link](https://arxiv.org/abs/2406.00800) | [Link](https://github.com/AozhongZhang/MagR)![](https://img.shields.io/github/stars/AozhongZhang/MagR.svg?style=social) |
| 2024 | AffineQuant: Affine Transformation Quantization for Large Language Models | ICLR 2024 | [Link](https://arxiv.org/pdf/2403.12544) |[Link](https://github.com/bytedance/AffineQuant)![](https://img.shields.io/github/stars/bytedance/AffineQuant.svg?style=social)|
| 2024 |LLM-QAT: Data-Free Quantization Aware Training for Large Language Models|ACL 2024|[Link](https://arxiv.org/pdf/2305.17888)|[Link](https://github.com/facebookresearch/LLM-QAT)![](https://img.shields.io/github/stars/facebookresearch/LLM-QAT.svg?style=social)|
| 2024 | BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation | ACL 2024 | [Link](https://arxiv.org/abs/2402.10631) | [Link](https://github.com/DD-DuDa/BitDistiller)![](https://img.shields.io/github/stars/DD-DuDa/BitDistiller.svg?style=social) |
| 2023 | OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models | AAAI 2024 (Oral) | [Link](https://arxiv.org/abs/2306.02272) | [Link](https://github.com/xvyaward/owq)![](https://img.shields.io/github/stars/xvyaward/owq.svg?style=social) |
| 2024 | SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression | ICLR 2024 | [Link](https://arxiv.org/abs/2306.03078) | [Link](https://github.com/Vahe1994/SpQR)![](https://img.shields.io/github/stars/Vahe1994/SpQR.svg?style=social) |
| 2022 | ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers | NeurIPS 2022 | [Link](https://arxiv.org/abs/2206.01861) | [Link](https://github.com/microsoft/DeepSpeed)![](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg?style=social) |
| 2024 | LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models | ICLR 2024        | [Link](https://arxiv.org/abs/2206.09557) | [Link](https://github.com/naver-aics/lut-gemm)![](https://img.shields.io/github/stars/naver-aics/lut-gemm.svg?style=social) |
| 2024 | OneBit: Towards Extremely Low-bit Large Language Models | NeurIPS 2024 | [Link](https://arxiv.org/abs/2402.11295) | [Link](https://github.com/xuyuzhuang11/OneBit)![](https://img.shields.io/github/stars/xuyuzhuang11/OneBit.svg?style=social) |
| 2023 | LLM-FP4: 4-bit Floating-Point Quantized Transformers | EMNLP 2023 | [Link](https://arxiv.org/abs/2310.16836) | [Link](https://github.com/nbasyl/LLM-FP4)![](https://img.shields.io/github/stars/nbasyl/LLM-FP4.svg?style=social) |
| 2024 | FlatQuant: Flatness Matters for LLM Quantization | ICML 2025 | [Link](https://arxiv.org/pdf/2410.09426) | [Link](https://github.com/ruikangliu/FlatQuant)![](https://img.shields.io/github/stars/ruikangliu/FlatQuant.svg?style=social) |
| 2024 | SqueezeLLM: Dense-and-Sparse Quantization | ICML 2024 | [Link](https://arxiv.org/pdf/2306.07629) | [Link](https://github.com/SqueezeAILab/SqueezeLLM)![](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM.svg?style=social) |
| 2023 | RPTQ: Reorder-based Post-training Quantization for Large Language Models |  | [Link](https://arxiv.org/pdf/2304.01089) | [Link](https://github.com/hahnyuan/RPTQ4LLM)![](https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM.svg?style=social) |
| 2024 | QQQ: Quality Quattuor-Bit Quantization for Large Language Models | ICLR | [Link](https://arxiv.org/pdf/2406.09904) | [Link](https://github.com/HandH1998/QQQ)![](https://img.shields.io/github/stars/HandH1998/QQQ.svg?style=social) |
| 2024 | Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs |                  | [Link](https://arxiv.org/pdf/2405.14428) | [Link](https://github.com/onnoo/activation-spikes)![](https://img.shields.io/github/stars/onnoo/activation-spikes.svg?style=social) |


## VLM Quantization
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                        |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2024 | Q-VLM: Post-training Quantization for Large Vision Language Models      | NIPS 2024 | [Link](https://arxiv.org/pdf/2410.08119) | [Link](https://github.com/ChangyuanWang17/QVLM) ![](https://img.shields.io/github/stars/ChangyuanWang17/QVLM.svg?style=social) |
| 2025 | MBQ:Modality-Balanced Quantization for Large Vision-Language Models     | CVPR 2025 | [Link](https://arxiv.org/pdf/2412.19509) | [Link](https://github.com/thu-nics/MBQ) ![](https://img.shields.io/github/stars/thu-nics/MBQ.svg?style=social) |
| 2025 | MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization     | ACM MM 2025 | [Link](https://arxiv.org/pdf/2502.00425) | [Link](https://github.com/StiphyJay/MQuant) ![](https://img.shields.io/github/stars/StiphyJay/MQuant.svg?style=social) |
| 2025 | CASP: Compression of Large Multimodal Models Based on Attention Sparsity     | CVPR 2025 | [Link](https://arxiv.org/pdf/2503.05936) | [Link](https://github.com/vbdi/casp) ![](https://img.shields.io/github/stars/vbdi/casp.svg?style=social) |



# Knowledge Distillation
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                        |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2025 | Random Conditioning with Distillation for Data-Efficient Diffusion Model Compression | CVPR 2025 | [Link](https://arxiv.org/abs/2504.02011) | [Link](https://github.com/dohyun-as/Random-Conditioning) ![](https://img.shields.io/github/stars/dohyun-as/Random-Conditioning.svg?style=social) |
| 2025 | LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation              | ICLR 2025 | [Link](https://arxiv.org/pdf/2408.15881) | [Link](https://github.com/shufangxun/LLaVA-MoD) ![](https://img.shields.io/github/stars/shufangxun/LLaVA-MoD.svg?style=social) |
| 2024 | PromptKD: Prompt-based Knowledge Distillation for Large Language Models | EMNLP 2024 | [Link](https://arxiv.org/abs/2405.12345) | [Link](https://github.com/example/PromptKD) ![](https://img.shields.io/github/stars/example/PromptKD.svg?style=social) |
| 2023 | AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression | ACL 2023 | [Link](https://arxiv.org/abs/2305.10010) | [Link](https://github.com/brucewsy/AD-KD) ![](https://img.shields.io/github/stars/brucewsy/AD-KD.svg?style=social) |
| 2023 | DiffKD: Diffusion-based Knowledge Distillation for Large Language Models | NIPS 2023 | [Link](https://arxiv.org/abs/2306.78901) | [Link](https://github.com/example/DiffKD) ![](https://img.shields.io/github/stars/example/DiffKD.svg?style=social) |
| 2023 | SCOTT: Self-Consistent Chain-of-Thought Distillation | ACL 2023 | [Link](https://arxiv.org/abs/2305.01879) | [Link](https://github.com/wangpf3/consistent-CoT-distillation) ![](https://img.shields.io/github/stars/wangpf3/consistent-CoT-distillation.svg?style=social) |
| 2023 | Distilling Script Knowledge from Large Language Models for Constrained Language Planning | ACL 2023 | [Link](https://arxiv.org/abs/2305.05252) | [Link](https://github.com/siyuyuan/coscript) ![](https://img.shields.io/github/stars/siyuyuan/coscript.svg?style=social) |
| 2023 | DOT: A Distillation-Oriented Trainer | ICCV 2023 | [Link](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DOT_A_Distillation-Oriented_Trainer_ICCV_2023_paper.pdf) | [Link](https://github.com/megvii-research/mdistiller) ![](https://img.shields.io/github/stars/megvii-research/mdistiller.svg?style=social) |
| 2022 | TinyViT: Fast Pretraining Distillation for Small Vision Transformers | ECCV 2022 | [Link](https://arxiv.org/pdf/2207.10666) | [Link](https://github.com/wkcn/tinyvit) ![](https://img.shields.io/github/stars/wkcn/tinyvit.svg?style=social) |
| 2022 | DIST: Distilling Large Language Models with Small-Scale Data | NIPS 2022 | [Link](https://arxiv.org/abs/2207.12345) | [Link](https://github.com/example/DIST) ![](https://img.shields.io/github/stars/example/DIST.svg?style=social) |
| 2022 | Decoupled Knowledge Distillation | CVPR 2022 | [Link](https://arxiv.org/abs/2203.08679) | [Link](https://github.com/megvii-research/mdistiller) ![](https://img.shields.io/github/stars/megvii-research/mdistiller.svg?style=social) |
| 2021 | HRKD: Hierarchical Relation-based Knowledge Distillation | EMNLP 2021 | [Link](https://arxiv.org/abs/2109.12345) | [Link](https://github.com/example/HRKD) ![](https://img.shields.io/github/stars/example/HRKD.svg?style=social) |
| 2021 | Distilling Knowledge via Knowledge Review | CVPR 2021 | [Link](https://arxiv.org/abs/2104.09044) | [Link](https://github.com/dvlab-research/ReviewKD) ![](https://img.shields.io/github/stars/dvlab-research/ReviewKD.svg?style=social) |
| 2023 | Specializing Smaller Language Models towards Multi-Step Reasoning | ICML 2023 | [Link](https://arxiv.org/abs/2301.12726) | [Link](https://github.com/FranxYao/FlanT5-CoT-Specialization) ![](https://img.shields.io/github/stars/FranxYao/FlanT5-CoT-Specialization.svg?style=social) |
| 2023 | Distilling Script Knowledge from Large Language Models for Constrained Language Planning | ACL 2023 | [Link](https://arxiv.org/abs/2305.05252) | [Link](https://github.com/siyuyuan/coscript) ![](https://img.shields.io/github/stars/siyuyuan/coscript.svg?style=social) |
| 2023 | DISCO: Distilling Counterfactuals with Large Language Models | ACL 2023 | [Link](https://arxiv.org/abs/2212.10534) | [Link](https://github.com/eric11eca/disco) ![](https://img.shields.io/github/stars/eric11eca/disco.svg?style=social) |
| 2023 | Can Language Models Teach? Teacher Explanations Improve Student Performance via Theory of Mind | NIPS 2023 | [Link](https://arxiv.org/abs/2306.09299) | [Link](https://github.com/swarnaHub/ExplanationIntervention) ![](https://img.shields.io/github/stars/swarnaHub/ExplanationIntervention.svg?style=social) |
| 2023 | PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation | EMNLP 2023 | [Link](https://arxiv.org/abs/2310.14192) | [Link](https://github.com/ServiceNow/PromptMix-EMNLP-2023) ![](https://img.shields.io/github/stars/ServiceNow/PromptMix-EMNLP-2023.svg?style=social) |
| 2024 | Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data | AAAI 2024 | [Link](https://arxiv.org/abs/2312.12832) | [Link](https://github.com/Yiwei98/TDG) ![](https://img.shields.io/github/stars/Yiwei98/TDG.svg?style=social) |
| 2023 | Democratizing Reasoning Ability: Tailored Learning from Large Language Model | EMNLP 2023 | [Link](https://aclanthology.org/2023.emnlp-main.120.pdf) | [Link](https://github.com/Raibows/Learn-to-Reason) ![](https://img.shields.io/github/stars/Raibows/Learn-to-Reason.svg?style=social) |
| 2023 | GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model | ACL 2023 | [Link](https://arxiv.org/abs/2306.06629) | [Link](https://github.com/aitsc/GLMKD) ![](https://img.shields.io/github/stars/aitsc/GLMKD.svg?style=social) |
| 2023 | Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes | ACL 2023 | [Link](https://arxiv.org/abs/2305.02301) | [Link](https://github.com/google-research/distilling-step-by-step) ![](https://img.shields.io/github/stars/google-research/distilling-step-by-step.svg?style=social) |
| 2023 | Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models | EMNLP 2023 | [Link](https://arxiv.org/abs/2310.13395) | [Link](https://github.com/stogiannidis/OCaTS) ![](https://img.shields.io/github/stars/stogiannidis/OCaTS.svg?style=social) |
| 2020 | Few Sample Knowledge Distillation for Efficient Network Compression | CVPR 2020 | [Link](https://arxiv.org/abs/1812.01839) | [Link](https://github.com/LTH14/FSKD) ![](https://img.shields.io/github/stars/LTH14/FSKD.svg?style=social) |
---
# Low-Rank Decomposition
| Year | Title                                                                   | Venue   | Paper                                 | code                                                                                                                        |
| ---- | ----------------------------------------------------------------------- | ------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 2024 | Compressing Large Language Models using Low Rank and Low Precision Decomposition | NeurIPS 2024 | [Link](https://openreview.net/pdf?id=lkx3OpcqSZ) | [Link](https://github.com/pilancilab/caldera) ![](https://img.shields.io/github/stars/pilancilab/caldera.svg?style=social) |
| 2022 | Compressible-composable NeRF via Rank-residual Decomposition | NeurIPS 2022 | [Link](https://proceedings.neurips.cc/paper_files/paper/2022/file/5ed5c3c846f684a54975ad7a2525199f-Paper-Conference.pdf) | [Link](https://github.com/ashawkey/CCNeRF) ![](https://img.shields.io/github/stars/ashawkey/CCNeRF.svg?style=social) |
| 2024 | Unified Low-rank Compression Framework for Click-through Rate Prediction | KDD 2024 | [Link](https://arxiv.org/abs/2405.18146) | [Link](https://github.com/yuhao318/Atomic_Feature_Mimicking) ![](https://img.shields.io/github/stars/yuhao318/Atomic_Feature_Mimicking.svg?style=social) |
| 2025 | Pivoting Factorization: A Compact Meta Low-Rank Representation of Sparsity for Efficient Inference in Large Language Models | ICML 2025 | [Link](https://icml.cc/virtual/2025/poster/46433) | [Link](https://github.com/biomedical-cybernetics/pivoting-factorization) ![](https://img.shields.io/github/stars/biomedical-cybernetics/pivoting-factorization.svg?style=social) |
| 2024 | SliceGPT: Orthogonal Slicing for Parameter-Efficient Transformer Compression | ICLR 2024 | [Link](https://arxiv.org/abs/2401.15024) | [Link](https://github.com/microsoft/TransformerCompression) ![](https://img.shields.io/github/stars/microsoft/TransformerCompression.svg?style=social) |
| 2024 | Low-Rank Knowledge Decomposition for Medical Foundation Models | CVPR 2024 | [Link](https://arxiv.org/abs/2409.19540) | [Link](https://github.com/MediaBrain-SJTU/LoRKD) ![](https://img.shields.io/github/stars/MediaBrain-SJTU/LoRKD.svg?style=social) |
| 2024 | LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking | CVPR 2024 | [Link](https://arxiv.org/abs/2403.04303) | [Link](https://github.com/li-jl16/LORS) ![](https://img.shields.io/github/stars/li-jl16/LORS.svg?style=social) |
| 2021 | Decomposable-Net: Scalable Low-Rank Compression for Neural Networks | IJCAI 2021 | [Link](https://www.ijcai.org/proceedings/2021/447) | [Link](https://github.com/ygcats/Scalable-Low-Rank-Compression-for-Neural-Networks) ![](https://img.shields.io/github/stars/ygcats/Scalable-Low-Rank-Compression-for-Neural-Networks.svg?style=social) |

---
# KV Cache Compression
| Year | Title                                                        | Venue        | Paper                                                                 | Code                                                                                                      | Category               |
|------|--------------------------------------------------------------|--------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|------------------------|
| 2023 | H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models | NeurIPS 2023 | [Link](https://arxiv.org/abs/2306.14048) | [Link](https://github.com/FMInference/H2O) ![](https://img.shields.io/github/stars/FMInference/H2O.svg?style=social) | Token Eviction |
| 2023 | Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time | NeurIPS 2023 | [Link](https://arxiv.org/abs/2305.17118) | [Link](https://github.com/lzcea/Scissorhands) ![](https://img.shields.io/github/stars/lzcea/Scissorhands.svg?style=social) | Token Eviction |
| 2023 | Efficient Streaming Language Models with Attention Sinks | ICLR 2024 | [Link](https://arxiv.org/abs/2309.17453) | [Link](https://github.com/mit-han-lab/streaming-llm) ![](https://img.shields.io/github/stars/mit-han-lab/streaming-llm.svg?style=social) | Token Eviction |
| 2024 | SnapKV: LLM Knows What You are Looking for Before Generation | NeurIPS 2024 | [Link](https://arxiv.org/abs/2404.14469) | [Link](https://github.com/FasterDecoding/SnapKV) ![](https://img.shields.io/github/stars/FasterDecoding/SnapKV.svg?style=social) | Token Eviction |
| 2024 | InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory | NeurIPS 2024 | [Link](https://arxiv.org/abs/2402.04617) | [Link](https://github.com/thunlp/InfLLM) ![](https://img.shields.io/github/stars/thunlp/InfLLM.svg?style=social) | Token Eviction |
| 2025 | InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation |  | [Link](https://arxiv.org/abs/2509.24663) | [Link](https://github.com/OpenBMB/infllmv2_cuda_impl) ![](https://img.shields.io/github/stars/OpenBMB/infllmv2_cuda_impl.svg?style=social) | Token Eviction |
| 2024 | Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs | ICLR 2024 | [Link](https://arxiv.org/abs/2310.01801) | [Link](https://github.com/machilusZ/FastGen) ![](https://img.shields.io/github/stars/machilusZ/FastGen.svg?style=social) | Token Eviction |
| 2024 | Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference | MLSys 2024 | [Link](https://arxiv.org/abs/2403.09054) | [Link](https://github.com/d-matrix-ai/keyformer-llm) ![](https://img.shields.io/github/stars/d-matrix-ai/keyformer-llm.svg?style=social) | Token Eviction |
| 2024 | Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference | ICML 2024 | [Link](https://arxiv.org/abs/2406.10774) | [Link](https://github.com/mit-han-lab/quest) ![](https://img.shields.io/github/stars/mit-han-lab/quest.svg?style=social) | Token Eviction |
| 2024 | On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference |  | [Link](https://arxiv.org/abs/2402.06262) | [Link](https://github.com/DRSY/EasyKV) ![](https://img.shields.io/github/stars/DRSY/EasyKV.svg?style=social) | Token Eviction |
| 2025 | R-KV: Redundancy-aware KV Cache Compression for Reasoning Models |  | [Link](https://arxiv.org/abs/2505.24133) | [Link](https://github.com/Zefan-Cai/R-KV) ![](https://img.shields.io/github/stars/Zefan-Cai/R-KV.svg?style=social) | Token Eviction |
| 2025 | SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator | ICML 2025 | [Link](https://arxiv.org/abs/2412.12094) | [Link](https://github.com/HKUDS/SepLLM) ![](https://img.shields.io/github/stars/HKUDS/SepLLM.svg?style=social) | Token Eviction |
| 2024 | RazorAttention: Efficient KV Cache Compression Through Retrieval Heads | ICLR 2025 | [Link](https://arxiv.org/abs/2407.15891) | N/A | Token Eviction |
| 2025 | Squeezed Attention: Accelerating Long Context Length LLM Inference | ACL 2025 | [Link](https://arxiv.org/abs/2411.09688) | [Link](https://github.com/SqueezeAILab/SqueezedAttention) ![](https://img.shields.io/github/stars/SqueezeAILab/SqueezedAttention.svg?style=social) | Token Eviction |
| 2024 | PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling |  | [Link](https://arxiv.org/abs/2406.02069v4) | [Link](https://github.com/Zefan-Cai/KVCache-Factory) ![](https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory.svg?style=social) | Budget Allocation |
| 2024 | VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration | ICLR 2025 | [Link](https://arxiv.org/abs/2410.23317) | N/A | Budget Allocation |
| 2025 | LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models | ICML 2025 | [Link](https://arxiv.org/abs/2507.14204) | [Link](https://github.com/GATECH-EIC/LaCache) ![](https://img.shields.io/github/stars/GATECH-EIC/LaCache.svg?style=social) | Budget Allocation |
| 2025 | CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences | ICLR 2025 | [Link](https://arxiv.org/abs/2503.12491) | [Link](https://github.com/antgroup/cakekv) ![](https://img.shields.io/github/stars/antgroup/cakekv.svg?style=social) | Budget Allocation |
| 2024 | MiniCache: KV Cache Compression in Depth Dimension for Large Language Models | NeurIPS 2024 | [Link](https://arxiv.org/abs/2405.14366) | [Link](https://github.com/AkideLiu/MiniCache) ![](https://img.shields.io/github/stars/AkideLiu/MiniCache.svg?style=social) | Cache Merging |
| 2024 | CaM: Cache Merging for Memory-efficient LLMs Inference | ICML 2024 | [Link](https://openreview.net/forum?id=LCTmppB165) | [Link](https://github.com/zyxxmu/cam) ![](https://img.shields.io/github/stars/zyxxmu/cam.svg?style=social) | Cache Merging |
| 2024 | Compressed Context Memory For Online Language Model Interaction | ICLR 2024 | [Link](https://arxiv.org/abs/2312.03414) | [Link](https://github.com/snu-mllab/context-memory) ![](https://img.shields.io/github/stars/snu-mllab/context-memory.svg?style=social) | Cache Merging |
| 2024 | Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference | ICML 2024 | [Link](https://arxiv.org/abs/2403.09636) | N/A | Cache Merging |
| 2024 | LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference | EMNLP 2024 Findings | [Link](https://arxiv.org/abs/2406.18139) | [Link](https://github.com/SUSTechBruce/LOOK-M) ![](https://img.shields.io/github/stars/SUSTechBruce/LOOK-M.svg?style=social) | Cache Merging |
| 2024 | CHAI: Clustered Head Attention for Efficient LLM Inference | ICML 2024 | [Link](https://arxiv.org/abs/2403.08058) | [Link](https://github.com/facebookresearch/chai) ![](https://img.shields.io/github/stars/facebookresearch/chai.svg?style=social) | Cache Merging |
| 2024 | D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models | ICLR 2025 | [Link](https://arxiv.org/abs/2406.13035) | [Link](https://github.com/AIoT-MLSys-Lab/D2O) ![](https://img.shields.io/github/stars/AIoT-MLSys-Lab/D2O.svg?style=social) | Cache Merging |
| 2025 | AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning | ICCV 2025 | [Link](https://arxiv.org/abs/2412.03248) | [Link](https://github.com/LaVi-Lab/AIM) ![](https://img.shields.io/github/stars/LaVi-Lab/AIM.svg?style=social) | Cache Merging |
| 2024 | IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact | ACL 2024 | [Link](https://arxiv.org/abs/2403.01241) | [Link](https://github.com/ruikangliu/IntactKV) ![](https://img.shields.io/github/stars/ruikangliu/IntactKV.svg?style=social) | Quantization |
| 2024 | KIVI: A Tuning-Free Asyetric 2bit Quantization for KV Cache | ICML 2024 | [Link](https://arxiv.org/abs/2402.02750) | [Link](https://github.com/jy-yuan/KIVI) ![](https://img.shields.io/github/stars/jy-yuan/KIVI.svg?style=social) | Quantization |
| 2024 | KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization | NeurIPS 2024 | [Link](https://arxiv.org/abs/2401.18079) | [Link](https://github.com/SqueezeAILab/KVQuant) ![](https://img.shields.io/github/stars/SqueezeAILab/KVQuant.svg?style=social) | Quantization |
| 2024 | SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models | COLM 2024 | [Link](https://arxiv.org/abs/2405.06219) | [Link](https://github.com/cat538/SKVQ) ![](https://img.shields.io/github/stars/cat538/SKVQ.svg?style=social) | Quantization |
| 2024 | GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM | NeurIPS 2024 | [Link](https://arxiv.org/abs/2403.05527) | [Link](https://github.com/opengear-project/GEAR) ![](https://img.shields.io/github/stars/opengear-project/GEAR.svg?style=social) | Quantization |
| 2024 | MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache | ACL 2025 | [Link](https://arxiv.org/abs/2411.18077) | N/A | Quantization |
| 2024 | Palu: Compressing KV-Cache with Low-Rank Projection | ICLR 2025 | [Link](https://arxiv.org/abs/2407.21118) | [Link](https://github.com/shadowpa0327/Palu) ![](https://img.shields.io/github/stars/shadowpa0327/Palu.svg?style=social) | Low Rank Projection |
| 2024 | Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning | ICLR 2025 | [Link](https://arxiv.org/abs/2410.19258) | [Link](https://github.com/FYYFU/HeadKV) ![](https://img.shields.io/github/stars/FYYFU/HeadKV.svg?style=social) | Token Eviction |
| 2024 | NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time | ACL 2024 | [Link](https://arxiv.org/abs/2408.03675) | N/A | Token Eviction |
| 2024 | SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation | ACL 2025 | [Link](https://arxiv.org/abs/2412.13649) | [Link](https://github.com/Linking-ai/SCOPE) ![](https://img.shields.io/github/stars/Linking-ai/SCOPE.svg?style=social) | Token Eviction |
| 2024 | AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asyetric Quantization Configurations | ACL 2025 | [Link](https://arxiv.org/abs/2410.13212) | N/A | Quantization |



---
# Speculative Decoding
| Year | Title                                                        | Venue    | Paper                                    | code                                                         |
| ---- | ------------------------------------------------------------ | -------- | ---------------------------------------- | ------------------------------------------------------------ |
| 2024 | Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting|NeurIPS 2024|[Kangaroo](https://arxiv.org/abs/2404.18911) | [code](https://github.com/Equationliu/Kangaroo) ![](https://img.shields.io/github/stars/Equationliu/Kangaroo.svg?style=social)|
| 2024 | EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees|EMNLP 2024|[EAGLE2](https://arxiv.org/abs/2406.16858) | [code](https://github.com/SafeAILab/EAGLE) ![](https://img.shields.io/github/stars/SafeAILab/EAGLE.svg?style=social)|
| 2025 |Learning Harmonized Representations for Speculative Sampling|ICLR 2025|[HASS](https://arxiv.org/pdf/2408.15766) |[code](https://github.com/HArmonizedSS/HASS) ![](https://img.shields.io/github/stars/HArmonizedSS/HASS.svg?style=social)|
| 2025 |Parallel Speculative Decoding with Adaptive Draft Length|ICLR 2025|[PEARL](https://arxiv.org/pdf/2408.11850) |[code](https://github.com/smart-lty/ParallelSpeculativeDecoding) ![](https://img.shields.io/github/stars/smart-lty/ParallelSpeculativeDecoding.svg?style=social)|
| 2025 |SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration|ICLR 2025|[SWIFT](https://openreview.net/pdf?id=EKJhH5D5wA) |[code](https://github.com/hemingkx/SWIFT) ![](https://img.shields.io/github/stars/hemingkx/SWIFT.svg?style=social)|
| 2025 |Pre-Training Curriculum for Multi-Token Prediction in Language Models|ACL 2025 |[paper](https://github.com/aynetdia/mtp_curriculum) | [code](https://github.com/aynetdia/mtp_curriculum) ![](https://img.shields.io/github/stars/aynetdia/mtp_curriculum.svg?style=social)|
| 2025 |Faster Speculative Decoding via Effective Draft Decoder with Pruned Candidate Tree|ACL 2025 |[paper](https://aclanthology.org/2025.acl-long.486.pdf) | N/A |


# Diffusion Models

## Quantization
| Year | Title | Venue | Task | Paper | Code |
|------|-------|-------|------|-------|------|
| 2025 | SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models | ICLR 2025 | T2I | [Link](https://arxiv.org/pdf/2411.05007) | [Link](https://github.com/nunchaku-tech/nunchaku) ![](https://img.shields.io/github/stars/nunchaku-tech/nunchaku.svg?style=social) |
| 2025 | ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation | ICLR 2025 | Image Generation | [Link](https://arxiv.org/pdf/2406.02540) | [Link](https://github.com/thu-nics/ViDiT-Q) ![](https://img.shields.io/github/stars/thu-nics/ViDiT-Q.svg?style=social) |
| 2023 | Post-training Quantization on Diffusion Models | CVPR 2023 | T2I、T2V | [Link](https://arxiv.org/pdf/2211.15736) | [Link](https://github.com/42Shawn/PTQ4DM) ![](https://img.shields.io/github/stars/42Shawn/PTQ4DM.svg?style=social) |
| 2023 | Q-Diffusion: Quantizing Diffusion Models | ICCV 2023 | Image Generation | [Link](https://arxiv.org/pdf/2302.04304) | [Link](https://github.com/Xiuyu-Li/q-diffusion) ![](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion.svg?style=social) |
| 2024 | Towards Accurate Post-training Quantization for Diffusion Models | CVPR 2024 | Image Generation | [Link](https://arxiv.org/pdf/2305.18723) | [Link](https://github.com/ChangyuanWang17/APQ-DM) ![](https://img.shields.io/github/stars/ChangyuanWang17/APQ-DM.svg?style=social) |
| 2024 | EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models | ICLR 2024 | Image Generation | [Link](https://arxiv.org/pdf/2310.03270) | [Link](https://github.com/ThisisBillhe/EfficientDM) ![](https://img.shields.io/github/stars/ThisisBillhe/EfficientDM.svg?style=social) |
| 2025 | Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers | CVPR 2025 | T2I、T2V | [Link](https://arxiv.org/pdf/2406.17343) | [Link](https://github.com/Juanerx/Q-DiT) ![](https://img.shields.io/github/stars/Juanerx/Q-DiT.svg?style=social) |
| 2024 | TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models | CVPR 2024 | Image Generation | [Link](https://arxiv.org/pdf/2311.16503) | [Link](https://github.com/ModelTC/TFMQ-DM) ![](https://img.shields.io/github/stars/ModelTC/TFMQ-DM.svg?style=social) |
| 2023 | Temporal Dynamic Quantization for Diffusion Models | NIPS 2023 |  | [Link](https://arxiv.org/pdf/2306.02316v2) | [Link](https://github.com/ECoLab-POSTECH/TDQ_NeurIPS2023) ![](https://img.shields.io/github/stars/ECoLab-POSTECH/TDQ_NeurIPS2023.svg?style=social) |
| 2024 | PTQ4DiT: Post-training Quantization for Diffusion Transformers | NIPS 2024 | T2I | [Link](https://arxiv.org/pdf/2405.16005) | [Link](https://github.com/adreamwu/PTQ4DiT) ![](https://img.shields.io/github/stars/adreamwu/PTQ4DiT.svg?style=social) |
| 2025 | Data-free Video Diffusion Transformers Quantization |  | T2V | [Link](https://arxiv.org/pdf/2505.18663) | [Link](https://github.com/lhxcs/DVD-Quant) ![](https://img.shields.io/github/stars/lhxcs/DVD-Quant.svg?style=social) |
| 2025 | DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing | WACV 2025 | T2V | [Link](https://arxiv.org/pdf/2409.07756) | [Link](https://github.com/DZY122/DiTAS) ![](https://img.shields.io/github/stars/DZY122/DiTAS.svg?style=social) |
| 2025 | Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs |  |  T2T | [Link](https://arxiv.org/pdf/2508.14896) |  |
| 2025 | SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration | ICLR 2025 | T2I、T2V | [Link](https://arxiv.org/pdf/2410.02367) | [Link](https://github.com/thu-ml/SageAttention) ![](https://img.shields.io/github/stars/thu-ml/SageAttention.svg?style=social) |
---

## Sparsity
| Year | Title | Venue | Task | Paper | Code |
|------|-------|-------|------|-------|------|
| 2024 | DiTFastAttn: Attention Compression for Diffusion Transformer Models | NIPS 2024 | T2I、T2V | [Link](https://arxiv.org/pdf/2406.08552) | [Link](https://github.com/thu-nics/DiTFastAttn) ![](https://img.shields.io/github/stars/thu-nics/DiTFastAttn.svg?style=social) |
| 2025 | Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity | ICML 2025 | T2V | [Link](https://arxiv.org/pdf/2502.01776) | [Link](https://github.com/svg-project/Sparse-VideoGen) ![](https://img.shields.io/github/stars/svg-project/Sparse-VideoGen.svg?style=social) |
| 2025 | Radial Attention: Sparse Attention with Energy Decay for Long Video Generation | NIPS 2025 | T2V | [Link](https://arxiv.org/pdf/2506.19852) | [Link](https://github.com/mit-han-lab/radial-attention) ![](https://img.shields.io/github/stars/mit-han-lab/radial-attention.svg?style=social) |
| 2025 | XAttention: Block Sparse Attention with Antidiagonal Scoring | ICML 2025 | T2T、T2V | [Link](https://arxiv.org/pdf/2503.16428) | [Link](https://github.com/mit-han-lab/x-attention) ![](https://img.shields.io/github/stars/mit-han-lab/x-attention.svg?style=social) |

## Caching & Reuse
| Year | Title | Venue | Task | Paper | Code |
|------|-------|-------|------|-------|------|
| 2025 | Timestep Embedding Tells: It’s Time to Cache for Video Diffusion Model | CVPR 2025 | T2V | [Link](https://arxiv.org/pdf/2411.19108) | [Link](https://github.com/ali-vilab/TeaCache) ![](https://img.shields.io/github/stars/ali-vilab/TeaCache.svg?style=social) |
| 2025 | From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers | ICCV 2025 | T2V | [Link](https://arxiv.org/pdf/2503.06923) | [Link](https://github.com/Shenyi-Z/TaylorSeer) ![](https://img.shields.io/github/stars/Shenyi-Z/TaylorSeer.svg?style=social) |
| 2025 | Adaptive Caching for Faster Video Generation with Diffusion Transformers |  | T2V | [Link](https://arxiv.org/pdf/2411.02397) | [Link](https://github.com/AdaCache-DiT/AdaCache) ![](https://img.shields.io/github/stars/AdaCache-DiT/AdaCache.svg?style=social) |
| 2024 | DeepCache: Accelerating Diffusion Models for Free | CVPR 2024 | T2I | [Link](https://arxiv.org/pdf/2312.00858) | [Link](https://github.com/horseee/DeepCache) ![](https://img.shields.io/github/stars/horseee/DeepCache.svg?style=social) |
| 2025 | Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching | NIPS 2024 | T2I | [Link](https://arxiv.org/pdf/2406.01733) | [Link](https://github.com/horseee/learning-to-cache) ![](https://img.shields.io/github/stars/horseee/learning-to-cache.svg?style=social) |
